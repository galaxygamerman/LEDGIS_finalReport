\chapter{Results and Discussion}

\noindent
This chapter presents a comprehensive evaluation of the LEDGIS framework, validating its functional correctness, quantitative performance, and practical applicability. The section begins by detailing the experimental environment, followed by a multi-faceted analysis of the results obtained, including functional test cases, quantitative performance benchmarks, and a comparative analysis against existing systems. The chapter concludes with a critical discussion of the insights gained, challenges faced, and limitations identified, ultimately proposing a clear roadmap for future development.

\noindent
\section{Experimentation Details}

All functional and quantitative evaluations were conducted within a controlled testbed. This section meticulously details the hardware, software, and network architecture of this environment, which is essential for contextualizing and interpreting the performance results that follow.

\subsection{Hardware Environment}

The performance benchmarks were conducted on a modest, developer-grade hardware setup to establish a conservative baseline for system viability. The testbed comprised:

\begin{itemize}
    \item \textbf{CPU:} Intel Core i3-7100U
    \item \textbf{RAM:} 16GB
    \item \textbf{Storage:} M.2 SSD
\end{itemize}

The use of non-server-grade hardware, particularly a dual-core, low-power processor, is a critical variable. The performance data (e.g., latency, throughput) derived from this testbed should be interpreted as a performance floor or a conservative baseline, not the system's maximum capability. Any performance bottlenecks identified, such as the saturation point observed under concurrent load, are partially attributable to this resource-constrained environment. 

The system's ability to function effectively on this hardware strongly supports its feasibility, as performance would predictably and significantly improve with an enterprise-grade deployment.

\subsection{Software and Network Architecture}

The LEDGIS system is a multi-component, containerized application designed for modularity and scalability. The experimental setup employed a comprehensive software stack, beginning with the blockchain network built on Hyperledger Fabric v2.5, deployed in Docker containers. The network consisted of multiple peers, an orderer, and a Certificate Authority (CA), all operating within a controlled local environment. The CouchDB v3.2 database served as the state database for Fabric peers, enabling efficient storage and retrieval of world-state data.

For decentralized storage, a locally running IPFS daemon was integrated to handle evidence file storage and retrieval operations, ensuring data redundancy and persistence. The backend of the system was developed using Node.js (v18+) with the Express.js framework, serving as the central orchestrator that connected all components and interacted with the blockchain through the fabric-sdk-node interface. The frontend layer, implemented using Next.js, provided a seamless user interface for performing blockchain transactions, uploading files, and retrieving evidence records. All these components — Fabric, IPFS, backend, and frontend — were containerized using Docker and interconnected via a dedicated Docker bridge network to ensure isolation and consistent inter-service communication.

This microservice-based architecture implies that the measured API latency does not represent solely the “blockchain speed.” Instead, it reflects the cumulative time required for a series of interdependent service calls. 

For example, a single \texttt{POST /upload} operation involves several sequential processes — an initial HTTP request, local cryptographic operations, interaction with the IPFS daemon, and a complete transaction submission to the Fabric peer. This inherent “orchestration overhead” contributes significantly to the total measured latency, suggesting that the core components such as Fabric and IPFS are, in fact, more efficient than the aggregate timings might initially indicate.

\section{Results Obtained}

This section presents the results of the project, showing that the system works correctly (the “what”) and performs efficiently (the “how well”). The snapshots and test cases from the original report are included here as they provide key evidence supporting these results.

\subsection{Functional Validation and test case validation}

This section provides qualitative and empirical evidence that all critical system components and pipelines function as designed. Validation was performed at both the API level using \textbf{Postman} and the application level via \textbf{UI snapshots}.

\subsubsection{API-Level Test Case: User Authentication (RBAC)}

The system’s Role-Based Access Control (RBAC) mechanism was validated through a series of API tests. A successful request to the \texttt{/login} endpoint correctly authenticated a valid user and returned a JSON Web Token (JWT). A subsequent test involved accessing an admin-only route using this JWT in the authorization header. The backend middleware correctly validated the token and authorized access based on the "Admin" role. This confirms the correct implementation of the system’s security and RBAC model.

\subsubsection{API-Level Test Case: End-to-End Evidence Upload Pipeline}

A \texttt{POST} request was sent to the evidence upload API endpoint with a sample file. The test returned a 200 OK status and a JSON response containing the file’s unique hash (generated using SHA-256) and the IPFS Content Identifiers (CIDs) for the stored chunks. This JSON response serves as direct proof of the successful execution of the entire Evidence Upload and Secure Storage Pipeline. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/postman2.jpg}
    \captionsetup{justification=centering}
    \caption{Postman Test: Evidence Upload API returning success with hash and CID storage confirmation}
    \label{fig:postman_upload}
\end{figure}

A successful response indicates that all critical steps—(1) hashing, (2) AES encryption, (3) chunking, (4) upload to IPFS, (5) metadata creation, and (6) storing metadata on the ledger—completed successfully. Any failure in this sequence would have triggered an error, thereby confirming the integrity and robustness of the cryptographic and distributed storage pipeline.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/postman.jpg}
    \captionsetup{justification=centering}
    \caption{Postman Test: Metadata Retrieval showing hash, key, IV, and chunk references from ledger}
    \label{fig:postman_query_1}
\end{figure}

\subsubsection{API-Level Test Case: End-to-End Evidence Retrieval Pipeline}

A \texttt{GET} request was sent to the metadata retrieval endpoint using a known file hash as a key. The system successfully queried the Hyperledger Fabric ledger through the \texttt{getEvidence} chaincode function and returned the stored JSON metadata object. The object contained the file’s hash, AES key, IV, and chunk references, validating the first half of the Evidence Retrieval and Verification Pipeline. 

This test confirms that metadata stored on-chain remains persistent, immutable, and retrievable. Furthermore, the retrieved key, IV, and CIDs serve as inputs for subsequent decryption and integrity verification processes.

\subsubsection{Application-Level (Snapshot) Validation}

Snapshots of the client application confirm that the user interface functions as intended. The Evidence Commit Page provides an intuitive interface for initiating the upload process . Additionally, the IPFS Map visualization illustrates active peers and storage statistics, demonstrating that the local IPFS service is successfully connected to the global peer-to-peer network rather than operating in isolation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/gmap.jpg}
    \captionsetup{justification=centering}
    \caption{Global IPFS Map: A webpage showcasing the global ipfs storage system nodes along with the node statistics}
    \label{fig:postman_query_2}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/evUp.jpg}
    \captionsetup{justification=centering}
    \caption{Evidence Commit Page: Interface to submit new evidence to the ledger}
    \label{fig:postman_query_3}
\end{figure}

\vspace{0.15in}
\noindent

\subsection{Performance Analysis for Scalability}

This section analyzes the system's performance in terms of file processing and the overhead involved in data ingestion. Quantitative data has been drawn from the research phase of the project. The system demonstrates highly efficient and nearly linear scaling for local file operations such as hashing, encryption, and chunking. As observed in the performance benchmark, a five-megabyte file is processed almost instantly, while a one-hundred-sixty-megabyte file is completely processed in about 1.2 seconds. This behavior indicates a linear scalability trend, confirming that local cryptographic operations are not a system bottleneck, even for large evidence files. These tasks are CPU-bound and scale predictably with input size, which validates the system’s computational efficiency.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/bar.png}
    \captionsetup{justification=centering}
    \caption{System processing time for handling files of various sizes, demonstrating a linear performance scaling that supports large files}
    \label{fig:performance_bar_chart_1}
\end{figure}

A clear difference in latency was observed between data ingestion and retrieval operations. The average latency for the POST upload operation was approximately 10,489 milliseconds, whereas the average latency for the GET retrieval operation was around 3,449 milliseconds. This difference highlights the expected performance characteristics of the hybrid architecture. Local processing of even large files is completed in about 1.2 seconds, while the total upload duration of roughly 10.5 seconds accounts for the additional overhead of uploading encrypted chunks to the distributed IPFS network and submitting metadata transactions to the Hyperledger Fabric network for consensus and block commitment.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/performance_metrics.png}
    \captionsetup{justification=centering}
    \caption{A comparison of API response times, showing the higher latency of the ingestion process due to cryptographic and consensus overhead.}
    \label{fig:performance_bar_chart_2}
\end{figure}

This ten-second duration represents the deliberate cost of achieving immutability and trust through blockchain consensus. Attempting to store the entire file directly on-chain would lead to unmanageable block sizes, failed transaction propagation, and an unusable ledger. By isolating only the lightweight metadata—such as the hash, CIDs, and encryption key—on the blockchain, while storing the actual file data on IPFS, the system effectively separates time-intensive trust operations from fast local processing. This approach confirms that the hybrid architecture provides the right balance between scalability, integrity, and performance.

\section{Discussions}

This section interprets the findings of the project and discusses their broader implications, challenges, and identified limitations.

\subsection{Insights from the Study}

The study demonstrates that the proposed system achieves verifiable immutability of digital evidence. The upload process effectively acts as a digital notary, creating an immutable, time-stamped record of an evidence hash on the ledger. The retrieval pipeline then functions as the auditor, verifying the integrity of the evidence against this recorded hash each time it is accessed. Together, these processes establish a closed-loop, cryptographically secured chain of custody essential for judicial reliability.

Another key insight is the automation of the chain-of-custody process and the shift of trust from human oversight to automated, verifiable protocols. Historically, evidence handling relied heavily on manual logs and subjective verification, which were prone to human error. By using smart contracts and automated backend workflows, the framework eliminates human intervention in evidence logging and verification. This transition from human trust to protocol-based verification provides significant governance and efficiency benefits in legal and investigative workflows.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/scalability_test.jpg}
    \caption{System performance under concurrent load, illustrating the relationship between throughput and response time as user load increases}
    \label{fig:placeholder}
\end{figure}

\subsection{Challenges and Problems Faced}

The main technical difficulty encountered during implementation was the orchestration of multiple interdependent services, including the frontend, backend, blockchain network, state database, and decentralized storage. Managing data flow, handling cryptographic identities such as the application wallet, and ensuring proper error propagation across these containerized components required careful configuration and debugging. 

Another important challenge involved cryptographic key management. A practical decision was made in the prototype phase to store AES encryption keys and initialization vectors on-chain along with metadata. While this approach facilitated testing and validation, it is not considered secure for a production environment. This limitation emphasizes the need for a robust, external key management mechanism in future iterations to enhance overall system security and compliance.

\section{Conclusion of results}
This chapter presented a detailed validation of the LEDGIS framework. The functional tests confirmed that the cryptographic and distributed storage pipelines work correctly from end to end. The performance analysis verified the efficiency of the hybrid on-chain and off-chain design, showing that it can handle large files with predictable, linear scaling and reasonable upload times. Load testing established a performance baseline and identified the system’s limits, providing useful information for future improvements. The comparison study highlighted LEDGIS as a unique, domain-specific solution for legal and forensic chain-of-custody management. Finally, this chapter discussed key findings, technical challenges, and limitations such as prototype-level key management, outlining clear directions for future research and system enhancement.

